{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "id": "c6a97d3d",
      "cell_type": "markdown",
      "source": [
        "# Regularization Techniques: Ridge, Lasso, and Elastic Net\n",
        "Quick Revision + Interview Memory Hooks"
      ],
      "metadata": {
        "id": "c6a97d3d"
      }
    },
    {
      "id": "57e1beab",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ“š Detailed Revision Notes\n",
        "\n",
        "### 1. Why Regularization?\n",
        "- High-dimensional data â†’ model can **overfit**.\n",
        "- Adds **penalty** to large coefficients â†’ smooth predictions, better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Types of Regularization\n",
        "\n",
        "#### **Ridge Regression (L2)**\n",
        "- Penalty: `Î± * Î£(coefficientsÂ²)`\n",
        "- Shrinks coefficients but **never exactly zero**.\n",
        "- Keeps all features; good for **multicollinearity**.\n",
        "- Formula: `Cost = RSS + Î± * Î£(Î²áµ¢Â²)`\n",
        "\n",
        "#### **Lasso Regression (L1)**\n",
        "- Penalty: `Î± * Î£|coefficients|`\n",
        "- Some coefficients become **exactly zero** â†’ **feature selection**.\n",
        "- Best when only a few features are important.\n",
        "- Formula: `Cost = RSS + Î± * Î£|Î²áµ¢|`\n",
        "\n",
        "#### **Elastic Net**\n",
        "- Combines Ridge + Lasso penalties.\n",
        "- Controlled by:\n",
        "  - `Î±` â†’ total strength.\n",
        "  - `l1_ratio` â†’ mix between L1 & L2.\n",
        "- Formula: `Cost = RSS + Î± * (l1_ratio * Î£|Î²áµ¢| + (1-l1_ratio) * Î£(Î²áµ¢Â²))`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. When to Use\n",
        "| Technique   | Coefficients go to 0? | Best for |\n",
        "|-------------|-----------------------|----------|\n",
        "| **Ridge**   | No                    | Multicollinearity |\n",
        "| **Lasso**   | Yes                   | Feature selection |\n",
        "| **Elastic** | Some                  | Correlated features + selection |\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Hyperparameter Tuning\n",
        "- Use **cross-validation** to choose `Î±` (and `l1_ratio` for Elastic Net).\n",
        "- **High Î±** â†’ more regularization (simpler model).\n",
        "- **Low Î±** â†’ less regularization (risk of overfitting).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Python Example\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "```\n"
      ],
      "metadata": {
        "id": "57e1beab"
      }
    },
    {
      "id": "ced75757",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸš€ Interview Memory Hooks\n",
        "\n",
        "**Why?**  \n",
        "Prevent overfitting by adding *penalties* to large coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Ridge (L2)** â†’ \"Shrink but don't kill\"\n",
        "- Sum of squares penalty.\n",
        "- All features kept, smaller weights.\n",
        "- **Think:** Ridge = Reduce.\n",
        "\n",
        "### **2. Lasso (L1)** â†’ \"Kill & shrink\"\n",
        "- Sum of absolute values penalty.\n",
        "- Can remove features (coefficients = 0).\n",
        "- **Think:** Lasso = Leave some out.\n",
        "\n",
        "### **3. Elastic Net**\n",
        "- Combo of L1 + L2.\n",
        "- **Think:** Elastic = Equal mix.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Quick Formulas\n",
        "- Ridge: `RSS + Î±Î£(Î²Â²)`\n",
        "- Lasso: `RSS + Î±Î£|Î²|`\n",
        "- Elastic: `RSS + Î±[l1_ratioÎ£|Î²| + (1-l1_ratio)Î£(Î²Â²)]`\n",
        "\n",
        "---\n",
        "\n",
        "### Mnemonics\n",
        "- **Ridge â†’ Reduce, not remove.**\n",
        "- **Lasso â†’ Leave some out.**\n",
        "- **Elastic â†’ Equal mix.**\n"
      ],
      "metadata": {
        "id": "ced75757"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHimlKFTfyUR"
      },
      "id": "WHimlKFTfyUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regularization Techniques in Linear Regression**\n",
        "\n",
        "### 1. **Why Regularization?**\n",
        "\n",
        "* In high-dimensional data (**many features, fewer samples**), linear regression can **overfit** â€” sharp peaks in predictions, perfect accuracy on training, poor on test data.\n",
        "* Regularization adds a **penalty** to the modelâ€™s coefficients â†’ discourages extreme weights â†’ **reduces overfitting**.\n",
        "* Goal: **Smooth bends instead of sharp peaks** in predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Types of Regularization**\n",
        "\n",
        "#### **A. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* **Penalty term:** `Î± * Î£ (coefficientsÂ²)`\n",
        "* Shrinks coefficients towards zero, **but never exactly zero**.\n",
        "* **Good for:** Multicollinearity (correlated features).\n",
        "* **Effect:** All features remain in the model, but with smaller influence.\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + Î± \\* Î£ (Î²áµ¢Â²)\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* **Penalty term:** `Î± * Î£ |coefficients|`\n",
        "* Can **reduce some coefficients exactly to zero** â†’ **feature selection**.\n",
        "* **Good for:** Models where you suspect only a few features are important.\n",
        "* **Effect:** Creates a sparse model (simpler, fewer predictors).\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + Î± \\* Î£ |Î²áµ¢|\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Elastic Net (L1 + L2 Regularization)**\n",
        "\n",
        "* Combines **Ridge** and **Lasso** penalties.\n",
        "* Balances between **feature selection** (L1) and **coefficient shrinkage** (L2).\n",
        "* Controlled by two hyperparameters:\n",
        "\n",
        "  * `Î±` â†’ overall strength of regularization.\n",
        "  * `l1_ratio` â†’ proportion of L1 vs L2 penalty.\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + Î± \\* (l1\\_ratio \\* Î£ |Î²áµ¢| + (1 - l1\\_ratio) \\* Î£ Î²áµ¢Â²)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **When to Use What**\n",
        "\n",
        "* **Ridge:** Many features, most are useful.\n",
        "* **Lasso:** Many features, only a few important.\n",
        "* **Elastic Net:** Features are correlated + need feature selection.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Hyperparameter Tuning**\n",
        "\n",
        "* Use **cross-validation** to find best `Î±` (and `l1_ratio` for Elastic Net).\n",
        "* Too high `Î±` â†’ underfitting; too low â†’ overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Python Code Snippets**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also make this into a **one-page colorful PDF \"cheat sheet\"** for quick revision so you donâ€™t need to reopen the big notebook.\n",
        "Do you want me to make that?\n"
      ],
      "metadata": {
        "id": "2EowlojGf-o4"
      },
      "id": "2EowlojGf-o4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQQlMTY6f17n"
      },
      "id": "JQQlMTY6f17n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš€ Regularization â€“ Instant Recall for Interviews\n",
        "\n",
        "**Why?**\n",
        "\n",
        "> Prevent overfitting by adding *penalties* to big coefficients â†’ keeps model simple & generalizable.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Ridge (L2) â†’ â€œShrink but donâ€™t killâ€**\n",
        "\n",
        "* Penalty: **Sum of squares** of coefficients.\n",
        "* Coefficients **get smaller**, but never become zero.\n",
        "* Keeps **all features**, just reduces their impact.\n",
        "* Best for: **Multicollinearity** (correlated features).\n",
        "\n",
        "ğŸ’¡ *Think:* Ridge = Reduce.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Lasso (L1) â†’ â€œKill & shrinkâ€**\n",
        "\n",
        "* Penalty: **Sum of absolute values** of coefficients.\n",
        "* Can force some coefficients **exactly to zero** â†’ feature selection.\n",
        "* Best for: Sparse solutions (**few features matter**).\n",
        "\n",
        "ğŸ’¡ *Think:* Lasso = Lasso rope â†’ pulls useless features out.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Elastic Net â†’ â€œBest of both worldsâ€**\n",
        "\n",
        "* Combo of **L1 (Lasso)** + **L2 (Ridge)** penalties.\n",
        "* Controlled by:\n",
        "\n",
        "  * **Î±** â†’ overall regularization strength.\n",
        "  * **l1\\_ratio** â†’ how much L1 vs L2.\n",
        "* Best for: **Correlated features + need feature selection**.\n",
        "\n",
        "ğŸ’¡ *Think:* Elastic = Flexible balance.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Quick Formula Triggers\n",
        "\n",
        "* **Ridge:** Cost = RSS + Î±Î£(Î²Â²)\n",
        "* **Lasso:** Cost = RSS + Î±Î£|Î²|\n",
        "* **Elastic:** Cost = RSS + Î±\\[l1\\_ratioÎ£|Î²| + (1-l1\\_ratio)Î£(Î²Â²)]\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”‘ Interview Mnemonics\n",
        "\n",
        "* **Ridge â†’ Reduce, not remove.**\n",
        "* **Lasso â†’ Leave some out.**\n",
        "* **Elastic â†’ Equal mix.**\n",
        "* **Î± up â†’ simpler model, less variance.**\n",
        "* **Î± down â†’ complex model, more variance.**\n",
        "\n",
        "---\n",
        "\n",
        "I can also make a **1-page â€œInterview Sheetâ€** with bold keywords, mnemonics, and a small table so you can glance at it for 30 seconds and have it fresh in your head.\n",
        "\n",
        "Do you want me to make that table version? Thatâ€™s killer for quick memorization.\n"
      ],
      "metadata": {
        "id": "hNB94miKgMPy"
      },
      "id": "hNB94miKgMPy"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAAWQclZf1wZ"
      },
      "id": "KAAWQclZf1wZ",
      "execution_count": null,
      "outputs": []
    }
  ]
}