{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "id": "c6a97d3d",
      "cell_type": "markdown",
      "source": [
        "# Regularization Techniques: Ridge, Lasso, and Elastic Net\n",
        "Quick Revision + Interview Memory Hooks"
      ],
      "metadata": {
        "id": "c6a97d3d"
      }
    },
    {
      "id": "57e1beab",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📚 Detailed Revision Notes\n",
        "\n",
        "### 1. Why Regularization?\n",
        "- High-dimensional data → model can **overfit**.\n",
        "- Adds **penalty** to large coefficients → smooth predictions, better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Types of Regularization\n",
        "\n",
        "#### **Ridge Regression (L2)**\n",
        "- Penalty: `α * Σ(coefficients²)`\n",
        "- Shrinks coefficients but **never exactly zero**.\n",
        "- Keeps all features; good for **multicollinearity**.\n",
        "- Formula: `Cost = RSS + α * Σ(βᵢ²)`\n",
        "\n",
        "#### **Lasso Regression (L1)**\n",
        "- Penalty: `α * Σ|coefficients|`\n",
        "- Some coefficients become **exactly zero** → **feature selection**.\n",
        "- Best when only a few features are important.\n",
        "- Formula: `Cost = RSS + α * Σ|βᵢ|`\n",
        "\n",
        "#### **Elastic Net**\n",
        "- Combines Ridge + Lasso penalties.\n",
        "- Controlled by:\n",
        "  - `α` → total strength.\n",
        "  - `l1_ratio` → mix between L1 & L2.\n",
        "- Formula: `Cost = RSS + α * (l1_ratio * Σ|βᵢ| + (1-l1_ratio) * Σ(βᵢ²))`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. When to Use\n",
        "| Technique   | Coefficients go to 0? | Best for |\n",
        "|-------------|-----------------------|----------|\n",
        "| **Ridge**   | No                    | Multicollinearity |\n",
        "| **Lasso**   | Yes                   | Feature selection |\n",
        "| **Elastic** | Some                  | Correlated features + selection |\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Hyperparameter Tuning\n",
        "- Use **cross-validation** to choose `α` (and `l1_ratio` for Elastic Net).\n",
        "- **High α** → more regularization (simpler model).\n",
        "- **Low α** → less regularization (risk of overfitting).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Python Example\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "```\n"
      ],
      "metadata": {
        "id": "57e1beab"
      }
    },
    {
      "id": "ced75757",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🚀 Interview Memory Hooks\n",
        "\n",
        "**Why?**  \n",
        "Prevent overfitting by adding *penalties* to large coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Ridge (L2)** → \"Shrink but don't kill\"\n",
        "- Sum of squares penalty.\n",
        "- All features kept, smaller weights.\n",
        "- **Think:** Ridge = Reduce.\n",
        "\n",
        "### **2. Lasso (L1)** → \"Kill & shrink\"\n",
        "- Sum of absolute values penalty.\n",
        "- Can remove features (coefficients = 0).\n",
        "- **Think:** Lasso = Leave some out.\n",
        "\n",
        "### **3. Elastic Net**\n",
        "- Combo of L1 + L2.\n",
        "- **Think:** Elastic = Equal mix.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Quick Formulas\n",
        "- Ridge: `RSS + αΣ(β²)`\n",
        "- Lasso: `RSS + αΣ|β|`\n",
        "- Elastic: `RSS + α[l1_ratioΣ|β| + (1-l1_ratio)Σ(β²)]`\n",
        "\n",
        "---\n",
        "\n",
        "### Mnemonics\n",
        "- **Ridge → Reduce, not remove.**\n",
        "- **Lasso → Leave some out.**\n",
        "- **Elastic → Equal mix.**\n"
      ],
      "metadata": {
        "id": "ced75757"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHimlKFTfyUR"
      },
      "id": "WHimlKFTfyUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regularization Techniques in Linear Regression**\n",
        "\n",
        "### 1. **Why Regularization?**\n",
        "\n",
        "* In high-dimensional data (**many features, fewer samples**), linear regression can **overfit** — sharp peaks in predictions, perfect accuracy on training, poor on test data.\n",
        "* Regularization adds a **penalty** to the model’s coefficients → discourages extreme weights → **reduces overfitting**.\n",
        "* Goal: **Smooth bends instead of sharp peaks** in predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Types of Regularization**\n",
        "\n",
        "#### **A. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* **Penalty term:** `α * Σ (coefficients²)`\n",
        "* Shrinks coefficients towards zero, **but never exactly zero**.\n",
        "* **Good for:** Multicollinearity (correlated features).\n",
        "* **Effect:** All features remain in the model, but with smaller influence.\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + α \\* Σ (βᵢ²)\n",
        "\n",
        "---\n",
        "\n",
        "#### **B. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* **Penalty term:** `α * Σ |coefficients|`\n",
        "* Can **reduce some coefficients exactly to zero** → **feature selection**.\n",
        "* **Good for:** Models where you suspect only a few features are important.\n",
        "* **Effect:** Creates a sparse model (simpler, fewer predictors).\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + α \\* Σ |βᵢ|\n",
        "\n",
        "---\n",
        "\n",
        "#### **C. Elastic Net (L1 + L2 Regularization)**\n",
        "\n",
        "* Combines **Ridge** and **Lasso** penalties.\n",
        "* Balances between **feature selection** (L1) and **coefficient shrinkage** (L2).\n",
        "* Controlled by two hyperparameters:\n",
        "\n",
        "  * `α` → overall strength of regularization.\n",
        "  * `l1_ratio` → proportion of L1 vs L2 penalty.\n",
        "\n",
        "**Formula:**\n",
        "Cost = RSS + α \\* (l1\\_ratio \\* Σ |βᵢ| + (1 - l1\\_ratio) \\* Σ βᵢ²)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **When to Use What**\n",
        "\n",
        "* **Ridge:** Many features, most are useful.\n",
        "* **Lasso:** Many features, only a few important.\n",
        "* **Elastic Net:** Features are correlated + need feature selection.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Hyperparameter Tuning**\n",
        "\n",
        "* Use **cross-validation** to find best `α` (and `l1_ratio` for Elastic Net).\n",
        "* Too high `α` → underfitting; too low → overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Python Code Snippets**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also make this into a **one-page colorful PDF \"cheat sheet\"** for quick revision so you don’t need to reopen the big notebook.\n",
        "Do you want me to make that?\n"
      ],
      "metadata": {
        "id": "2EowlojGf-o4"
      },
      "id": "2EowlojGf-o4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQQlMTY6f17n"
      },
      "id": "JQQlMTY6f17n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Regularization – Instant Recall for Interviews\n",
        "\n",
        "**Why?**\n",
        "\n",
        "> Prevent overfitting by adding *penalties* to big coefficients → keeps model simple & generalizable.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Ridge (L2) → “Shrink but don’t kill”**\n",
        "\n",
        "* Penalty: **Sum of squares** of coefficients.\n",
        "* Coefficients **get smaller**, but never become zero.\n",
        "* Keeps **all features**, just reduces their impact.\n",
        "* Best for: **Multicollinearity** (correlated features).\n",
        "\n",
        "💡 *Think:* Ridge = Reduce.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Lasso (L1) → “Kill & shrink”**\n",
        "\n",
        "* Penalty: **Sum of absolute values** of coefficients.\n",
        "* Can force some coefficients **exactly to zero** → feature selection.\n",
        "* Best for: Sparse solutions (**few features matter**).\n",
        "\n",
        "💡 *Think:* Lasso = Lasso rope → pulls useless features out.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Elastic Net → “Best of both worlds”**\n",
        "\n",
        "* Combo of **L1 (Lasso)** + **L2 (Ridge)** penalties.\n",
        "* Controlled by:\n",
        "\n",
        "  * **α** → overall regularization strength.\n",
        "  * **l1\\_ratio** → how much L1 vs L2.\n",
        "* Best for: **Correlated features + need feature selection**.\n",
        "\n",
        "💡 *Think:* Elastic = Flexible balance.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Quick Formula Triggers\n",
        "\n",
        "* **Ridge:** Cost = RSS + αΣ(β²)\n",
        "* **Lasso:** Cost = RSS + αΣ|β|\n",
        "* **Elastic:** Cost = RSS + α\\[l1\\_ratioΣ|β| + (1-l1\\_ratio)Σ(β²)]\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 Interview Mnemonics\n",
        "\n",
        "* **Ridge → Reduce, not remove.**\n",
        "* **Lasso → Leave some out.**\n",
        "* **Elastic → Equal mix.**\n",
        "* **α up → simpler model, less variance.**\n",
        "* **α down → complex model, more variance.**\n",
        "\n",
        "---\n",
        "\n",
        "I can also make a **1-page “Interview Sheet”** with bold keywords, mnemonics, and a small table so you can glance at it for 30 seconds and have it fresh in your head.\n",
        "\n",
        "Do you want me to make that table version? That’s killer for quick memorization.\n"
      ],
      "metadata": {
        "id": "hNB94miKgMPy"
      },
      "id": "hNB94miKgMPy"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAAWQclZf1wZ"
      },
      "id": "KAAWQclZf1wZ",
      "execution_count": null,
      "outputs": []
    }
  ]
}